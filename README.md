### Setup
I used my generic environment for this project. You can create a conda env from `general_environment.yml` (this has extraneous packages since it's my "general" environment). You also need an openai key to run experiments.

```
conda env create -f general_environment.yml
export PYTHONPATH=<root directory of this repo>
export OPENAI_API_KEY=<your key here>
```

### Running experiments

To run the experiments on openai models, use `src/prompt_openai.py`. To run experiments on open-source models, please use `src/prompt_open_llms.py`. The arguments to these scripts are the same, so this is abbreviated to `src/prompt_XXX.py`. `src/prompt_open_llms.py` assumes someone is running the server on tir.

After each experiment runs, the accuracy will be logged and a csv file with the prompts/answers will be saved to `./logs` by default with an autogenerated file name (you can change this through `--output`). 

#### Toy experiments

There are toy experiments on SCAN and Colours domains. 

**SCAN**
I used the first 1000 examples in the SCAN test set. You can set end_ind to a lower number.

**Colours**
Just replace `scan` with `colours` in the arguments below.

Base prompt (in-context examples only):
`python src/prompt_XXX.py --model <model_name> --dataset scan --use_min_cover --end_ind 1000`

With ground-truth rules: 
`python src/prompt_XXX.py --model <model_name> --dataset scan --use_min_cover --end_ind 1000 --prompt_type full_grammar`

With self-induced rules:
`python src/prompt_XXX.py --model <model_name> --dataset scan --use_min_cover --end_ind 1000 --prompt_type grammar_induction`

#### Language translation 

**Cherokee** 
The Cherokee experiments are currently not working because of the limited vocab available. 

**NACLO**
Substitute `naclo` as the dataset above.

#### ConceptARC

